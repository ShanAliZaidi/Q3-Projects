{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1_la7pduFTudnDGsge8YybsI68NvWGwK_",
      "authorship_tag": "ABX9TyPGjnbNfaCoTTNGJJshCISr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShanAliZaidi/RAG_Project/blob/main/Rag_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Tz0tm5_uA4iT",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "%pip install -qU langchain-google-genai langchain_pinecone langchain-community pypdf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "from pinecone import Pinecone,ServerlessSpec\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "GEMINI_API_KEY = userdata.get(\"GEMINI_KEY\")\n",
        "pinecone_key = userdata.get(\"pinecone_key\")\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
        "os.environ[\"PINECONE_API_KEY\"] = pinecone_key\n",
        "\n",
        "#initalize chat model\n",
        "llm_model = ChatGoogleGenerativeAI(\n",
        "    model = \"gemini-2.0-flash-exp\",\n",
        "    temperature = 0\n",
        ")\n",
        "\n",
        "#initialize embeddings\n",
        "embeddings_model = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "\n",
        "#initialize vector store\n",
        "pc = Pinecone(api_key=pinecone_key)\n",
        "index_name = \"web-index\"\n",
        "existing_indexes = [index_info[\"name\"] for index_info in pc.list_indexes()]\n",
        "if index_name not in existing_indexes:\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=768,\n",
        "        metric=\"cosine\",\n",
        "        spec=ServerlessSpec(cloud=\"aws\", region=\"us-east-1\"),\n",
        "    )\n",
        "index = pc.Index(index_name)\n",
        "vector_store = PineconeVectorStore(embedding=embeddings_model, index=index)"
      ],
      "metadata": {
        "id": "7cl0qobHBvEV"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.document_loaders import TextLoader\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "page_url = \"https://python.langchain.com/docs/tutorials/rag/\"\n",
        "\n",
        "loader = WebBaseLoader(web_paths=[page_url])\n",
        "\n",
        "\n",
        "#loading the document\n",
        "#upload the file in the files menu on the left\n",
        "\n",
        "#file_path = \"/content/imambaqeras_lifesketchzhj.pdf\"\n",
        "#loader = PyPDFLoader(file_path)\n",
        "#loader = TextLoader(file_path)\n",
        "documents = loader.load()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35aq8RVIMSX4",
        "outputId": "35929ea9-d1b8-4e4b-ab8a-840e353ab3b7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:langchain_community.utils.user_agent:USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(f\"Total characters: {len(documents[0].page_content)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYqJ7wiPe1Hd",
        "outputId": "ec57a85f-c8cc-4e41-c44f-ed862b5442d7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total characters: 42732\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "#splitting document\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,  # chunk size (characters)\n",
        "    chunk_overlap=200,  # chunk overlap (characters)\n",
        "    add_start_index=True,  # track index in original document\n",
        ")\n",
        "docs = text_splitter.split_documents(documents)\n",
        "document_ids = vector_store.add_documents(documents=docs)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "NUFBOOEQOPMx"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Split blog post into {len(docs)} sub-documents.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TLU4JDj2eQ3k",
        "outputId": "a09b1e68-0507-4928-e055-89f0232da419"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split blog post into 67 sub-documents.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Create embeddings and upload to Pinecone\n",
        "for doc in tqdm(docs):\n",
        "    vector = embeddings_model.embed_query(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1KetZr9TfQdM",
        "outputId": "cba69cec-9a9f-4d62-ec95-bce51822874d"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 67/67 [00:08<00:00,  7.60it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "retriever = vector_store.as_retriever()"
      ],
      "metadata": {
        "id": "wyQy1wpogSm-"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "qa_chain = RetrievalQA.from_chain_type(\n",
        "    llm = llm_model,\n",
        "    chain_type = \"stuff\",\n",
        "    retriever = retriever,  # Pass the retriever here\n",
        "    return_source_documents = True  # Optional: to get the source documents used in the response\n",
        ")"
      ],
      "metadata": {
        "id": "k_0Nnizyg5LF"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "res = qa_chain.invoke(\"what is splitting the document and how we can do it\")\n",
        "print(res[\"result\"])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yp1_MuNjjtCN",
        "outputId": "2f5eed7e-069f-4273-8a1c-d0e6b63ecfe9"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Splitting a document refers to dividing a large document into smaller chunks. This is often necessary because many models have context window limitations, and even if they don't, they can struggle to find information in very long inputs. By splitting the document into smaller, more manageable pieces, it becomes easier to process and retrieve relevant information.\n",
            "\n",
            "You can split a document using a `RecursiveCharacterTextSplitter`. This method recursively splits the document using common separators like new lines until each chunk is the appropriate size. It is the recommended text splitter for generic text use cases.\n",
            "\n",
            "Here's how you can do it:\n",
            "\n",
            "```python\n",
            "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
            "\n",
            "text_splitter = RecursiveCharacterTextSplitter(\n",
            "    chunk_size=1000,  # chunk size (characters)\n",
            "    chunk_overlap=200,  # chunk overlap (characters)\n",
            "    add_start_index=True,  # track index in original document\n",
            ")\n",
            "all_splits = text_splitter.split_documents(docs)\n",
            "print(f\"Split blog post into {len(all_splits)} sub-documents.\")\n",
            "```\n",
            "\n",
            "In this code:\n",
            "\n",
            "-   `chunk_size` specifies the maximum size of each chunk in characters.\n",
            "-   `chunk_overlap` defines the number of overlapping characters between consecutive chunks.\n",
            "-   `add_start_index` indicates whether to track the starting index of each chunk in the original document.\n",
            "-   `split_documents` is the method that performs the splitting on the input documents.\n",
            "\n"
          ]
        }
      ]
    }
  ]
}